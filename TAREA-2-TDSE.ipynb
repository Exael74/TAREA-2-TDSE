{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe647b4c",
   "metadata": {},
   "source": [
    "# Heart Disease Risk Prediction – Logistic Regression (from scratch)\n",
    "\n",
    "Tarea práctica de regresión logística sin usar scikit-learn para el entrenamiento (solo NumPy/Pandas/Matplotlib). Incluye EDA, modelos básicos, regularización y notas de despliegue en Amazon SageMaker.\n",
    "\n",
    "- Dataset: UCI Heart Disease (303 filas, 14 columnas), descarga desde Kaggle (`https://www.kaggle.com/datasets/neurocipher/heartdisease`).\n",
    "\n",
    "- Objetivo: predecir presencia de enfermedad cardiaca (`target` binario).\n",
    "\n",
    "- Entregables: notebook ejecutable con gráficos/tablas y README con resumen y evidencias de despliegue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae626a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías principales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Limpiar cache de fuentes corrupto (soluciona \"font not found\")\n",
    "try:\n",
    "    import shutil\n",
    "    cache_dir = matplotlib.get_cachedir()\n",
    "    fontlist_file = Path(cache_dir) / 'fontlist-v330.json'\n",
    "    if fontlist_file.exists():\n",
    "        fontlist_file.unlink()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Recargar font_manager para reconstruir fuentes\n",
    "matplotlib.font_manager.fontManager = matplotlib.font_manager.FontManager()\n",
    "\n",
    "# Configurar matplotlib para evitar símbolos LaTeX problemáticos\n",
    "plt.rcParams['text.usetex'] = False\n",
    "plt.rcParams['mathtext.default'] = 'regular'\n",
    "\n",
    "# Configuración de gráficos con fallback robusto\n",
    "for style_name in ['seaborn-v0_8', 'seaborn', 'ggplot', 'default']:\n",
    "    try:\n",
    "        plt.style.use(style_name)\n",
    "        break\n",
    "    except Exception:\n",
    "        continue\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_PATH = Path('Heart_Disease_Prediction.csv')\n",
    "\n",
    "# Ajustes para display\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b484c63",
   "metadata": {},
   "source": [
    "## Paso 1: Carga, exploración y preparación de datos\n",
    "- Cargamos del CSV y verificación de columnas.\n",
    "- Binarización del `target` (1 = enfermedad, 0 = sano).\n",
    "- Estadísticos descriptivos, valores faltantes y distribución de clases.\n",
    "- División estratificada 70/30 y normalización (z-score) de variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92773d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset\n",
    "assert DATA_PATH.exists(), f\"No se encontró el archivo {DATA_PATH}\"\n",
    "raw_df = pd.read_csv(DATA_PATH)\n",
    "# Normalizar nombres de columnas a minúsculas y sin espacios\n",
    "raw_df.columns = raw_df.columns.str.strip().str.lower()\n",
    "print(f\"Shape original: {raw_df.shape}\")\n",
    "print(f\"Columnas normalizadas: {list(raw_df.columns)}\")\n",
    "\n",
    "# Detectar y binarizar columna objetivo\n",
    "possible_targets = ['target', 'num']\n",
    "target_col = None\n",
    "for col in possible_targets:\n",
    "    if col in raw_df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "if target_col is None:\n",
    "    target_col = raw_df.columns[-1]  # fallback al último\n",
    "\n",
    "# Binarizar la columna objetivo aunque venga como texto\n",
    "col = raw_df[target_col]\n",
    "col_numeric = pd.to_numeric(col, errors='coerce')\n",
    "if col_numeric.notna().all():\n",
    "    target_clean = (col_numeric > 0).astype(int)\n",
    "else:\n",
    "    col_lower = col.astype(str).str.strip().str.lower()\n",
    "    map_dict = {\n",
    "        '1': 1, 'yes': 1, 'y': 1, 'true': 1, 'present': 1, 'disease': 1, 'presence': 1,\n",
    "        '0': 0, 'no': 0, 'n': 0, 'false': 0, 'absent': 0, 'absence': 0\n",
    "    }\n",
    "    target_mapped = col_lower.map(map_dict)\n",
    "    if target_mapped.isna().any():\n",
    "        uniques = col_lower.unique()\n",
    "        raise ValueError(f\"No se pudo binarizar target. Revisa valores: {uniques}\")\n",
    "    target_clean = target_mapped.astype(int)\n",
    "\n",
    "raw_df[target_col] = target_clean\n",
    "raw_df = raw_df.rename(columns={target_col: 'target'})\n",
    "\n",
    "# Vista rápida\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA básica\n",
    "summary = raw_df.describe().T\n",
    "missing = raw_df.isnull().sum().rename('missing')\n",
    "class_counts = raw_df['target'].value_counts().sort_index()\n",
    "class_pct = (class_counts / len(raw_df)).rename('pct') * 100\n",
    "\n",
    "print(\"Valores faltantes por columna:\")\n",
    "display(pd.concat([missing, summary[['mean', 'std', 'min', 'max']]], axis=1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "class_counts.plot(kind='bar', ax=ax, color=['tab:blue','tab:orange'])\n",
    "ax.set_xticklabels(['No enfermedad (0)','Enfermedad (1)'], rotation=0)\n",
    "ax.set_ylabel('Número de registros')\n",
    "ax.set_title('Distribución de la variable objetivo')\n",
    "for p, pct in zip(ax.patches, class_pct):\n",
    "    ax.annotate(f\"{pct:.1f}%\", (p.get_x() + p.get_width()/2, p.get_height()+1), ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División estratificada 70/30 y normalización\n",
    "\n",
    "def stratified_train_test_split(df, test_size=0.3, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    train_idx, test_idx = [], []\n",
    "    for label, group in df.groupby('target'):\n",
    "        idx = group.index.to_numpy()\n",
    "        np.random.shuffle(idx)\n",
    "        split = int(len(idx) * (1 - test_size))\n",
    "        train_idx.extend(idx[:split])\n",
    "        test_idx.extend(idx[split:])\n",
    "    train_df = df.loc[train_idx].sample(frac=1, random_state=random_state)\n",
    "    test_df = df.loc[test_idx].sample(frac=1, random_state=random_state)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Filtrar solo columnas numéricas\n",
    "all_features = [c for c in raw_df.columns if c != 'target' and pd.api.types.is_numeric_dtype(raw_df[c])]\n",
    "\n",
    "def pick_features(candidates=('age','trestbps','chol','thalach','oldpeak','ca')):\n",
    "    available = [c for c in candidates if c in all_features]\n",
    "    if len(available) >= 6:\n",
    "        return available[:6]\n",
    "    # fallback: take first 6 numeric features available\n",
    "    return all_features[:min(6, len(all_features))]\n",
    "\n",
    "selected_features = pick_features()\n",
    "print(f\"Usando características: {selected_features}\")\n",
    "\n",
    "train_df, test_df = stratified_train_test_split(raw_df, test_size=0.3, random_state=123)\n",
    "\n",
    "# Normalización z-score sobre train\n",
    "feature_mean = train_df[selected_features].mean()\n",
    "feature_std = train_df[selected_features].std().replace(0, 1)\n",
    "\n",
    "def transform(df):\n",
    "    x = (df[selected_features] - feature_mean) / feature_std\n",
    "    y = df['target'].values.reshape(-1, 1)\n",
    "    return x.values, y\n",
    "\n",
    "X_train, y_train = transform(train_df)\n",
    "X_test, y_test = transform(test_df)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Implementación de regresión logística desde cero\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def compute_cost(w, b, X, y, reg_lambda=0.0):\n",
    "    m = X.shape[0]\n",
    "    z = X @ w + b\n",
    "    a = sigmoid(z)\n",
    "    epsilon = 1e-8  # estabilidad numérica\n",
    "    cost = - (1/m) * np.sum(y * np.log(a + epsilon) + (1 - y) * np.log(1 - a + epsilon))\n",
    "    if reg_lambda > 0:\n",
    "        cost += (reg_lambda / (2*m)) * np.sum(w**2)\n",
    "    return cost\n",
    "\n",
    "\n",
    "def compute_gradients(w, b, X, y, reg_lambda=0.0):\n",
    "    m = X.shape[0]\n",
    "    z = X @ w + b\n",
    "    a = sigmoid(z)\n",
    "    dz = a - y\n",
    "    dw = (1/m) * (X.T @ dz) + (reg_lambda/m) * w\n",
    "    db = (1/m) * np.sum(dz)\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.01, num_iters=1500, reg_lambda=0.0, verbose=False):\n",
    "    n_features = X.shape[1]\n",
    "    w = np.zeros((n_features, 1))\n",
    "    b = 0.0\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dw, db = compute_gradients(w, b, X, y, reg_lambda)\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        if i % 50 == 0 or i == num_iters - 1:\n",
    "            cost = compute_cost(w, b, X, y, reg_lambda)\n",
    "            cost_history.append(cost)\n",
    "            if verbose and i % 200 == 0:\n",
    "                print(f\"Iter {i}: cost={cost:.4f}\")\n",
    "    return w, b, cost_history\n",
    "\n",
    "\n",
    "def predict_proba(w, b, X):\n",
    "    return sigmoid(X @ w + b)\n",
    "\n",
    "\n",
    "def predict_labels(w, b, X, threshold=0.5):\n",
    "    return (predict_proba(w, b, X) >= threshold).astype(int)\n",
    "\n",
    "\n",
    "def classification_metrics(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    m = len(y_true)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento base (sin regularización)\n",
    "alpha = 0.01\n",
    "num_iters = 1500\n",
    "w_base, b_base, cost_history = gradient_descent(X_train, y_train, alpha=alpha, num_iters=num_iters, reg_lambda=0.0, verbose=False)\n",
    "\n",
    "# Predicciones y métricas\n",
    "train_pred = predict_labels(w_base, b_base, X_train)\n",
    "test_pred = predict_labels(w_base, b_base, X_test)\n",
    "train_metrics = classification_metrics(y_train, train_pred)\n",
    "test_metrics = classification_metrics(y_test, test_pred)\n",
    "\n",
    "print(\"Métricas (train vs test):\")\n",
    "metrics_df = pd.DataFrame([train_metrics, test_metrics], index=['train','test']).round(3)\n",
    "display(metrics_df)\n",
    "\n",
    "# Curva de costo\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(np.arange(len(cost_history))*50, cost_history, marker='o')\n",
    "ax.set_xlabel('Iteraciones')\n",
    "ax.set_ylabel('Costo')\n",
    "ax.set_title('Curva de costo (sin regularización)')\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Coeficientes para interpretación\n",
    "coef_df = pd.DataFrame({'feature': selected_features, 'weight': w_base.reshape(-1)})\n",
    "coef_df.sort_values('weight', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Visualización de fronteras de decisión (pares de características)\n",
    "\n",
    "# Elegir pares solo con columnas numéricas disponibles\n",
    "avail = [c for c in raw_df.columns if c != 'target' and pd.api.types.is_numeric_dtype(raw_df[c])]\n",
    "if len(avail) < 2:\n",
    "    raise ValueError(\"No hay suficientes columnas numéricas para crear pares\")\n",
    "if len(avail) >= 6:\n",
    "    pair_candidates = [(avail[0], avail[1]), (avail[2], avail[3]), (avail[4], avail[5])]\n",
    "elif len(avail) >= 4:\n",
    "    pair_candidates = [(avail[0], avail[1]), (avail[2], avail[3])]\n",
    "else:\n",
    "    pair_candidates = [(avail[0], avail[1])]\n",
    "print(f\"Usando pares: {pair_candidates}\")\n",
    "\n",
    "\n",
    "def train_on_pair(pair, reg_lambda=0.0, alpha=0.05, num_iters=2500):\n",
    "    f1, f2 = pair\n",
    "    if f1 not in raw_df.columns or f2 not in raw_df.columns:\n",
    "        raise ValueError(f\"Faltan columnas para el par {pair}\")\n",
    "    mean_pair = train_df[[f1, f2]].mean()\n",
    "    std_pair = train_df[[f1, f2]].std().replace(0, 1)\n",
    "\n",
    "    def transform_pair(df):\n",
    "        Xp = (df[[f1, f2]] - mean_pair) / std_pair\n",
    "        yp = df['target'].values.reshape(-1, 1)\n",
    "        return Xp.values, yp\n",
    "\n",
    "    Xtr, ytr = transform_pair(train_df)\n",
    "    Xte, yte = transform_pair(test_df)\n",
    "    w, b, cost_hist = gradient_descent(Xtr, ytr, alpha=alpha, num_iters=num_iters, reg_lambda=reg_lambda)\n",
    "    return w, b, cost_hist, Xtr, ytr, Xte, yte, mean_pair, std_pair\n",
    "\n",
    "\n",
    "def plot_boundary(ax, X, y, w, b, mean_pair, std_pair, title):\n",
    "    scatter = ax.scatter(X[:,0], X[:,1], c=y.reshape(-1), cmap='bwr', alpha=0.7, edgecolor='k')\n",
    "    x_min, x_max = X[:,0].min()-0.5, X[:,0].max()+0.5\n",
    "    y_min, y_max = X[:,1].min()-0.5, X[:,1].max()+0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    zz = predict_labels(w, b, grid).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, zz, alpha=0.2, cmap='bwr')\n",
    "    # Línea de probabilidad 0.5 (compatible con todas las versiones de matplotlib)\n",
    "    logits = (grid @ w + b).reshape(xx.shape)\n",
    "    ax.contour(xx, yy, logits, levels=[0], colors='k', linewidths=2)\n",
    "    # Añadir leyenda manualmente\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_line = Line2D([0], [0], color='k', linewidth=2, label='p=0.5')\n",
    "    ax.legend(handles=[legend_line])\n",
    "    ax.set_xlabel('Feature 1 (normalizada)')\n",
    "    ax.set_ylabel('Feature 2 (normalizada)')\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def evaluate_pair(pair, reg_lambda=0.0):\n",
    "    w, b, cost_hist, Xtr, ytr, Xte, yte, mean_pair, std_pair = train_on_pair(pair, reg_lambda=reg_lambda)\n",
    "    train_pred = predict_labels(w, b, Xtr)\n",
    "    test_pred = predict_labels(w, b, Xte)\n",
    "    metrics = {\n",
    "        'train': classification_metrics(ytr, train_pred),\n",
    "        'test': classification_metrics(yte, test_pred)\n",
    "    }\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    plot_boundary(ax, Xtr, ytr, w, b, mean_pair, std_pair, title=f\"Frontera {pair} (lambda={reg_lambda})\")\n",
    "    plt.show()\n",
    "\n",
    "    return metrics, cost_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Regularización L2 y búsqueda de lambda\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1.0]\n",
    "reg_results = []\n",
    "for lam in lambdas:\n",
    "    w_lam, b_lam, cost_hist = gradient_descent(X_train, y_train, alpha=alpha, num_iters=num_iters, reg_lambda=lam)\n",
    "    train_pred = predict_labels(w_lam, b_lam, X_train)\n",
    "    test_pred = predict_labels(w_lam, b_lam, X_test)\n",
    "    train_m = classification_metrics(y_train, train_pred)\n",
    "    test_m = classification_metrics(y_test, test_pred)\n",
    "    reg_results.append({\n",
    "        'lambda': lam,\n",
    "        'train_acc': train_m['accuracy'],\n",
    "        'test_acc': test_m['accuracy'],\n",
    "        'test_f1': test_m['f1'],\n",
    "        'w_norm': np.linalg.norm(w_lam)\n",
    "    })\n",
    "\n",
    "reg_df = pd.DataFrame(reg_results)\n",
    "display(reg_df.round(3))\n",
    "\n",
    "best_idx = reg_df['test_f1'].idxmax()\n",
    "best_lambda = reg_df.loc[best_idx, 'lambda']\n",
    "print(f\"Mejor lambda por F1 de test: {best_lambda}\")\n",
    "\n",
    "# Crear figura sin usar tight_layout para evitar problemas de fuentes\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Usar índice para evitar problemas con lambda=0 en escala log\n",
    "x_ticks = list(range(len(lambdas)))\n",
    "\n",
    "ax[0].plot(x_ticks, reg_df['test_f1'], marker='o', label='F1 test')\n",
    "ax[0].set_xticks(x_ticks)\n",
    "ax[0].set_xticklabels([str(l) for l in lambdas])\n",
    "ax[0].set_xlabel('lambda')\n",
    "ax[0].set_ylabel('F1')\n",
    "ax[0].set_title('F1 vs lambda')\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(x_ticks, reg_df['w_norm'], marker='s', color='tab:red')\n",
    "ax[1].set_xticks(x_ticks)\n",
    "ax[1].set_xticklabels([str(l) for l in lambdas])\n",
    "ax[1].set_xlabel('lambda')\n",
    "ax[1].set_ylabel('Norma w')  # Texto plano en lugar de ||w||\n",
    "ax[1].set_title('Magnitud de pesos vs lambda')\n",
    "ax[1].grid(True)\n",
    "\n",
    "# Usar subplots_adjust en lugar de tight_layout para evitar problema de fonts\n",
    "fig.subplots_adjust(left=0.08, right=0.95, wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación de frontera con y sin regularización para un par\n",
    "pair_to_compare = pair_candidates[0]\n",
    "\n",
    "w0, b0, _, Xtr0, ytr0, _, _, mean0, std0 = train_on_pair(pair_to_compare, reg_lambda=0.0)\n",
    "wreg, breg, _, Xtr1, ytr1, _, _, mean1, std1 = train_on_pair(pair_to_compare, reg_lambda=best_lambda)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "plot_boundary(axes[0], Xtr0, ytr0, w0, b0, mean0, std0, title=f\"{pair_to_compare} sin regularizacion\")\n",
    "plot_boundary(axes[1], Xtr1, ytr1, wreg, breg, mean1, std1, title=f\"{pair_to_compare} con lambda={best_lambda}\")\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
